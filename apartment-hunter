#!/usr/bin/env python3
import os
import requests
import time
import json
import argparse
import logging as l
from termcolor import colored
from datetime import datetime as dt
from datetime import timedelta
from bs4 import BeautifulSoup
from sheetfu import SpreadsheetApp, Table
from dataclasses import dataclass
from schema import Dump, Notification, NotificationAction, get_db


TOKEN_PATH = './secrets/sa.json'
with open('./secrets/sheet_name') as f:
  SHEET_NAME = f.read().strip()
LOCAL_SHEETS_DATA_FN = 'sheets_data.json'

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 12_3_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15'
    }
PULL_INTERVAL = 3600
THROTTLE_TIME = 1


def get_google_sheet():
  sa = SpreadsheetApp(TOKEN_PATH)
  return sa.open_by_id(SHEET_NAME)


def get_apartments_from_google_sheets(local=False):
  '''pull data from google sheets and save as JSON'''
  if local:
    l.info('Using local spreadsheet data')
    with open(LOCAL_SHEETS_DATA_FN) as f:
      return json.loads(f.read())
  else:
    l.info('Getting data from spreadsheet...')
    sp = get_google_sheet()
    s = sp.sheets[0]
    table = Table(s.get_data_range())

    data = [{k.lower(): v for k, v in a.to_dict().items()}
        for a in table]
    # to save as json, remove datetimes
    for d in data:
      d['start date'] = str(d['start date'])
    with open(LOCAL_SHEETS_DATA_FN, 'w') as f:
      f.write(json.dumps(data))
    return data


def get_updated_dump(a, dumps_by_url):
  '''get a new dump for apartment `a` if outdated'''
  cur_ts = dt.now().timestamp()
  delta = (timedelta(seconds=int(cur_ts - dumps_by_url[a['url']].timestamp))
      if a['url'] in dumps_by_url else 0)

  if not delta or delta.total_seconds() > PULL_INTERVAL:
    l.info(f"Pulling data for: {colored(a['name'], 'green')} after {str(delta)}")
    r = requests.get(a['url'], headers=HEADERS)
    d = Dump(0, a['url'], dt.now().timestamp(), r.status_code, r.text)
    return d

  else:
    l.info(f"Skipping {colored(a['name'], 'green')}: Last pulled {str(delta)} ago")
    return None


def extract_dump(d):
  ''''''
  bs = BeautifulSoup(d.body, features='lxml')
  res = []
  if 'apartments.com' in d.url:
    models = bs.select('#pricingView > div[data-tab-content-id="bed1"] .pricingGridItem')
    for m in models:
      model_name = m.select_one('.priceBedRangeInfo .modelName').text.strip()
      for u in m.select('.unitContainer'):
        # remove these tags
        for s in u.select('.screenReaderOnly'):
          s.extract()

        res.append({
          'model': model_name,
          'unit': u.select_one('.unitColumn').text.strip(),
          'price': int(u.select_one('.pricingColumn').text.strip()
            .replace('$', '').replace(',', '')
            ),
          'sqft': int(u.select_one('.sqftColumn').text.strip()
            .replace(',', '')
            ),
          'available': u.select_one('.availableColumn').text.strip(),
          })
  else:
    return None
  return res


def update_dumps(args):
  conn, c = get_db()
  
  # get latest dumps for each url
  res = c.execute('''
      SELECT t1.*
      FROM dumps t1
      JOIN (
        SELECT url, max(timestamp) as timestamp
        FROM dumps t2
        WHERE status = 200
        GROUP BY url
      ) t2 on t1.timestamp = t2.timestamp
      ''')
  dumps = [Dump(*r) for r in res]
  dumps_by_url = {d.url: d for d in dumps}

  # get apartments from google sheets
  apmts = get_apartments_from_google_sheets(local=args.local)
  for a in apmts:
    d = get_updated_dump(a, dumps_by_url)
    if d:
      # new dump pulled
      d.insert(conn, c)
      dumps_by_url[d.url] = d
      time.sleep(THROTTLE_TIME)
    else:
      d = dumps_by_url[a['url']]

    # extract data
    data = extract_dump(d)
    if data is not None:
      l.info(f'Found {len(data)} units')
      for u in data:
        l.info(f'{json.dumps(u)}')


def get_price_history(apmts, conn, c):
  hist, unit_data = {}, {}
  for i, a in enumerate(apmts):
    res = c.execute('''
        SELECT t1.*
        FROM dumps t1
        WHERE status = 200 AND url = ?
        ORDER BY timestamp
        ''', (a['url'],))

    for r in res:
      d = Dump(*r)
      data = extract_dump(d)

      for u in data:
        k = (i, u['model'], u['unit'])
        if k not in unit_data:
          unit_data[k] = u

        if k not in hist:
          hist[k] = []
        # elif hist[k][-1][0] == u['price']:
        #   continue

        hist[k].append((
          u['price'],
          dt.fromtimestamp(d.timestamp),
          # dt.fromtimestamp(d.timestamp).strftime('%b %-d %Y')
          ))
    
  hdata = sorted(hist.items(), key=lambda p: p[0])
  return hdata


def sync_price_history(args):
  '''sync price history by day to google sheets'''
  conn, c = get_db()
  apmts = get_apartments_from_google_sheets(True)
  hdata = get_price_history(apmts, conn, c)
  # for (index, model, unit), h in hdata:
  #   l.info(f"{colored(apmts[index]['name'], 'green')}: {model}/{unit}: {h}")

  dates = [t.date() for k, h in hdata for p, t in h]
  min_date, max_date = min(dates), max(dates)
  vals = [[None for n in range((max_date - min_date).days + 3)]
      for _ in range(len(hdata) + 1)]

  notifications = []
  for j, (k, h) in enumerate(hdata):
    index, model, unit = k
    vals[j+1][0] = apmts[index]['name']
    vals[j+1][1] = f'{model}/{unit}'

  for n in range((max_date - min_date).days + 1):
    d = min_date + timedelta(days=n)
    vals[0][n+2] = d.strftime('%b %-d %Y')

    for j, (k, h) in enumerate(hdata):
      p, t = next(((p, t) for p, t in h[::-1] if t.date() == d), (None, None))
      vals[j+1][n+2] = p

      if d == max_date and p is not None:
        nf = check_notify_price_change(conn, c, apmts, k, p, t)
        if nf:
          notifications.append(nf)

  if not args.dry_run:
    l.info('Updating spreadsheet')
    sp = get_google_sheet()
    s = sp.sheets[1]
    s.get_data_range().set_value(None)

    dr = s.get_range(row=1, column=1,
        number_of_row=len(vals), number_of_column=len(vals[0]))
    dr.set_values(vals)
    sp.commit()

  for n in notifications:
    l.info(f'Sending notification: {n}')
    if True or not args.dry_run:
      n.insert(conn, c)


def check_notify_price_change(conn, c, apmts, k, p, t):
  '''notify user on price change'''
  index, model, unit = k
  r = c.execute('''
      SELECT *
      FROM notifications
      WHERE name = ? AND unit = ?
      ORDER BY last_notified DESC
      LIMIT 1
      ''', (apmts[index]['name'], unit)).fetchone()
  action = None
  data = {'price': p}
  if r:
    n = Notification(*r)
    nd = json.loads(n.data)
    if nd['price'] != p:
      data['last_price'] = nd['price']
      action = (
          NotificationAction.PRICE_INCREASE if p > nd['price'] else
          NotificationAction.PRICE_DECREASE)
  else:
    action = NotificationAction.ADDED

  if action:
    return Notification(0, apmts[index]['name'], unit, int(t.timestamp()), action.name, json.dumps(data))
  return None


if __name__ == "__main__":
  l.basicConfig(level=l.INFO)
  # l.basicConfig(level=l.DEBUG)

  action_funcs = {
      'update': update_dumps,
      'history': sync_price_history,
      'migrate': lambda args: get_db(migrate=True),
      }

  parser = argparse.ArgumentParser(description='apartment hunter')
  parser.add_argument('action', choices=action_funcs.keys())
  parser.add_argument('--local',
      action="store_true", help='use local sheets data')
  parser.add_argument('--dry-run',
      action="store_true", help='dont make changes')
  args = parser.parse_args()
  action_funcs[args.action](args)
